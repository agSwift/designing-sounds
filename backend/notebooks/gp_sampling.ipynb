{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.NumericalWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_KEOPS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        if HAVE_KEOPS:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.RBFKernel())\n",
    "        else:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeewookim/.local/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:278: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
      "  curr_conjugate_vec,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.862   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.817   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.770   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.716   lengthscale: 0.554   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.668   lengthscale: 0.513   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.617   lengthscale: 0.474   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.576   lengthscale: 0.439   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.534   lengthscale: 0.408   noise: 0.402\n",
      "Iter 9/50 - Loss: 0.491   lengthscale: 0.380   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.453   lengthscale: 0.356   noise: 0.339\n",
      "Iter 11/50 - Loss: 0.414   lengthscale: 0.335   noise: 0.311\n",
      "Iter 12/50 - Loss: 0.376   lengthscale: 0.317   noise: 0.285\n",
      "Iter 13/50 - Loss: 0.339   lengthscale: 0.301   noise: 0.261\n",
      "Iter 14/50 - Loss: 0.298   lengthscale: 0.288   noise: 0.238\n",
      "Iter 15/50 - Loss: 0.263   lengthscale: 0.276   noise: 0.217\n",
      "Iter 16/50 - Loss: 0.225   lengthscale: 0.266   noise: 0.198\n",
      "Iter 17/50 - Loss: 0.184   lengthscale: 0.258   noise: 0.181\n",
      "Iter 18/50 - Loss: 0.155   lengthscale: 0.250   noise: 0.165\n",
      "Iter 19/50 - Loss: 0.126   lengthscale: 0.244   noise: 0.150\n",
      "Iter 20/50 - Loss: 0.090   lengthscale: 0.238   noise: 0.136\n",
      "Iter 21/50 - Loss: 0.054   lengthscale: 0.234   noise: 0.124\n",
      "Iter 22/50 - Loss: 0.021   lengthscale: 0.230   noise: 0.113\n",
      "Iter 23/50 - Loss: -0.007   lengthscale: 0.226   noise: 0.103\n",
      "Iter 24/50 - Loss: -0.032   lengthscale: 0.224   noise: 0.094\n",
      "Iter 25/50 - Loss: -0.061   lengthscale: 0.221   noise: 0.086\n",
      "Iter 26/50 - Loss: -0.085   lengthscale: 0.220   noise: 0.078\n",
      "Iter 27/50 - Loss: -0.108   lengthscale: 0.219   noise: 0.071\n",
      "Iter 28/50 - Loss: -0.119   lengthscale: 0.218   noise: 0.065\n",
      "Iter 29/50 - Loss: -0.141   lengthscale: 0.218   noise: 0.060\n",
      "Iter 30/50 - Loss: -0.157   lengthscale: 0.217   noise: 0.055\n",
      "Iter 31/50 - Loss: -0.159   lengthscale: 0.217   noise: 0.051\n",
      "Iter 32/50 - Loss: -0.173   lengthscale: 0.217   noise: 0.047\n",
      "Iter 33/50 - Loss: -0.178   lengthscale: 0.218   noise: 0.044\n",
      "Iter 34/50 - Loss: -0.176   lengthscale: 0.218   noise: 0.041\n",
      "Iter 35/50 - Loss: -0.179   lengthscale: 0.218   noise: 0.039\n",
      "Iter 36/50 - Loss: -0.169   lengthscale: 0.219   noise: 0.036\n",
      "Iter 37/50 - Loss: -0.177   lengthscale: 0.219   noise: 0.035\n",
      "Iter 38/50 - Loss: -0.166   lengthscale: 0.220   noise: 0.033\n",
      "Iter 39/50 - Loss: -0.157   lengthscale: 0.222   noise: 0.032\n",
      "Iter 40/50 - Loss: -0.169   lengthscale: 0.224   noise: 0.031\n",
      "Iter 41/50 - Loss: -0.170   lengthscale: 0.226   noise: 0.030\n",
      "Iter 42/50 - Loss: -0.159   lengthscale: 0.228   noise: 0.030\n",
      "Iter 43/50 - Loss: -0.150   lengthscale: 0.230   noise: 0.030\n",
      "Iter 44/50 - Loss: -0.159   lengthscale: 0.233   noise: 0.029\n",
      "Iter 45/50 - Loss: -0.163   lengthscale: 0.237   noise: 0.030\n",
      "Iter 46/50 - Loss: -0.161   lengthscale: 0.239   noise: 0.030\n",
      "Iter 47/50 - Loss: -0.167   lengthscale: 0.242   noise: 0.030\n",
      "Iter 48/50 - Loss: -0.159   lengthscale: 0.245   noise: 0.031\n",
      "Iter 49/50 - Loss: -0.165   lengthscale: 0.248   noise: 0.031\n",
      "Iter 50/50 - Loss: -0.176   lengthscale: 0.251   noise: 0.032\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "test_n = 10000\n",
    "\n",
    "test_x = torch.linspace(0, 1, test_n)\n",
    "if torch.cuda.is_available():\n",
    "    test_x = test_x.cuda()\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with CIQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeewookim/.local/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:278: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [1, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
      "  curr_conjugate_vec,\n",
      "/Users/jeewookim/.local/lib/python3.7/site-packages/gpytorch/utils/contour_integral_quad.py:86: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)\n",
      "  approx_eigs = lanczos_mat.symeig()[0]\n",
      "/Users/jeewookim/.local/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [1, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
      "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 787 ms, total: 3.38 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "\n",
    "test_x.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    # All relevant settings for using CIQ.\n",
    "    #   ciq_samples(True) - Use CIQ for sampling\n",
    "    #   num_contour_quadrature(10) -- Use 10 quadrature sites (Q in the paper)\n",
    "    #   minres_tolerance -- error tolerance from minres (here, <0.01%).\n",
    "    print(\"Running with CIQ\")\n",
    "    with gpytorch.settings.ciq_samples(True), gpytorch.settings.num_contour_quadrature(10), gpytorch.settings.minres_tolerance(1e-4):\n",
    "        %time y_samples = observed_pred.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_x = torch.linspace(0, 1, self.sample_rate)\n",
    "\n",
    "        f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        ax.plot(self.train_x, self.train_y, 'k*')\n",
    "        ax.set_ylim([-5, 7.5])\n",
    "\n",
    "        if is_sample:\n",
    "            ax.plot(test_x.numpy(), data)\n",
    "            ax.legend(['Data', 'Sample from posterior'])\n",
    "        else:\n",
    "            mean, (lower, upper) = data\n",
    "            ax.plot(test_x.numpy(), mean, 'b')\n",
    "            ax.fill_between(test_x.numpy(), lower, upper, alpha=0.5)\n",
    "            ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ff24d51068bbbcc82c46406452659ccbf843361a6b38ca27c1f1bd6b7606bb3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
